# -*- coding: utf-8 -*-
"""
Created on Sat Jan 14 21:44:03 2023

@author: Geoffrey Dolinger
"""


import argparse
import os
import fnmatch
import matplotlib.pyplot as plt
import numpy as np


from job_control import *

#################################################################
def create_parser():
    '''
    Create argument parser
    -This function will create the parser structure and include parameter 
    setting given the specific application
    '''
    # Parse the command-line arguments
    parser = argparse.ArgumentParser(description='DetTH', fromfile_prefix_chars='@')

    # High-level experiment configuration
    parser.add_argument('--exp_type', type=str, default=None, help="Experiment type")    
    parser.add_argument('--label', type=str, default=None, help="Extra label to add to output files")
   #parser.add_argument('--dataset', type=str, default='radiant_earth/pa', help='Data set directory')    
    parser.add_argument('--results_path', type=str, default='./results', help='Results directory')

    # Specific experiment configuration
    parser.add_argument('--exp_index', type=int, default=None, help='Experiment index')
    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--lrate', type=float, default=0.001, help="Learning rate")
    parser.add_argument('--image_size', nargs=3, type=int, default=[28,28,1], help="Size of input images (rows, cols, channels)")
    parser.add_argument('--nclasses', type=int, default=4, help='Number of classes in maxpool output')    
   
    #Individual layer parameters (convolution in this example)
    parser.add_argument('--conv_size', nargs='+', type=int, default=[3,3], help='kernel size of leading conv layers') 
    parser.add_argument('--conv_nfilters', nargs='+', type=int, default=[20,20], help='Convolution filters per layer (sequence of ints)')
    parser.add_argument('--conv_stride', nargs='+', type=int, default=[1,1], help='stride amount in conv layers') 
    parser.add_argument('--maxpool',type=int, default=2, help='size of pooling')
    parser.add_argument('--dropout_spatial', nargs='+', type=float, default=.1, help='Inception Layer compression flag')
    
    parser.add_argument('--dense', nargs='+', type=int, default=[100, 50], help='Number of hidden units per layer (sequence of ints)')
    parser.add_argument('--dropout_dense', type=float, default=None, help='Dropout rate')
    parser.add_argument('--L2_dense', type=float, default=None, help="L2 regularization parameter")
    # Early stopping
    parser.add_argument('--min_delta', type=float, default=0.0, help="Minimum delta for early termination")
    parser.add_argument('--patience', type=int, default=40, help="Patience for early termination")

    # Training parameters
    parser.add_argument('--batch', type=int, default=8, help="Training set batch size")
    parser.add_argument('--steps_per_epoch', type=int, default=1000, help="Number of gradient descent steps per epoch")    
    
    
    
    return parser

def generate_fname(args, params_str):
    '''
    Generate the base file name for output files/directories.
    
    The approach is to encode the key experimental parameters in the file name.  This
    way, they are unique and easy to identify after the fact.

    :param args: from argParse
    :params_str: String generated by the JobIterator
    
    This function will generate file naming related to the experiment.  Any 
    parameter in the args parser can be updated to be part of the naming 
    convention.
    '''
    
    # Conv configuration
    if args.conv_size is None:
        conv_str = ''
        conv_size_str = ''
        conv_filter_str = ''
    else:
        conv_str = 'Conv'
        conv_size_str = '_'.join(str(x) for x in args.conv_size)
        conv_filter_str = '_'.join(str(x) for x in args.conv_nfilters)   
    Dense
    if args.dense is None:
        dense_str = ''
        dense_size_str = ''
    else:
        dense_str = 'Dense'
        dense_size_str = '_'.join(str(x) for x in args.dense)
    # Dropout
    if args.dropout_dense is None:
        dropout_str = ''
    else:
        dropout_str = 'drop_%0.3f_'%(args.dropout_dense)
    # Label
    if args.label is None:
        label_str = ""
    else:
        label_str = "%s_"%args.label
    # # Conv dropout
    if args.dropout_spatial is None:
        dropoutS_str = ''
    else:
        dropoutS_str = 'dropS_%0.3f_'%(args.dropout_spatial)
        dropoutS_str= dropout_str.replace('.','_')
     
     # L2 regularization
    if args.L2_dense is None:
        regularizer_l2_str = ''
    else:
        regularizer_l2_str = 'L2_%0.6f_'%(args.lambda_l2)
        regularizer_l2_str= regularizer_l2_str.replace('.','_')    
    # Experiment type
    if args.exp_type is None:
        experiment_type_str = ""
    else:
        experiment_type_str = "%s_"%args.exp_type

    # learning rate
    lrate_str = "LR_%0.6f_"%args.lrate
    lrate_str= lrate_str.replace('.','_')
    # Put it all together, include a %s for each included string or argument
    return "%s/%s%s_%s%s_%s_%s%s"%(args.results_path,
                                        experiment_type_str, label_str,                            
                                        dense_str, dense_size_str,
                                        dropout_str, 
                                        regularizer_l2_str,                                                                           
                                        lrate_str)
    '''
    Other naming examples:
    # Inception
    if args.inception is None:
        incep_str =''
    else:
        incep_str = '_'.join(str(x) for x in args.inception)
    
    
        
    # Conv dropout
    if args.dropout_spatial is None:
        dropout_str = ''
    else:
        dropout_str = 'drop_%0.3f_'%(args.dropout_spatial)
        dropout_str= dropout_str.replace('.','_')
    
    # L2 regularization
    if args.lambda_l2 is None:
        regularizer_l2_str = ''
    else:
        regularizer_l2_str = 'L2_%0.6f_'%(args.lambda_l2)
        regularizer_l2_str= regularizer_l2_str.replace('.','_')
    '''

#################################################################
def augment_args(args):
    '''
    Use the jobiterator to override the specified arguments based on the experiment index.

    Modifies the args

    :param args: arguments from ArgumentParser
    :return: A string representing the selection of parameters to be used in the file name
    '''
    
    # Create parameter sets to execute the experiment on.  This defines the Cartesian product
    #  of experiments that we will be executing
    p = exp_type_to_hyperparameters(args)

    # Check index number
    index = args.exp_index
    if(index is None):
        return ""
    
    # Create the iterator, Function within job_control.py, comment away if 
    # not using job_control for multiple jobs.
    ji = JobIterator(p)
    print("Total jobs:", ji.get_njobs())
    
    # Check bounds
    assert (args.exp_index >= 0 and args.exp_index < ji.get_njobs()), "exp_index out of range"

    # Print the parameters specific to this exp_index
    print(ji.get_index(args.exp_index))
    
    # Push the attributes to the args object and return a string that describes these structures
    return ji.set_attributes_by_index(args.exp_index, args)


def execute_exp(args=None):
    '''
    Perform the training and evaluation for a single model
    
    :param args: Argparse arguments
    '''
    

    # Check the arguments
    if args is None:
        # Case where no args are given (usually, because we are calling from within Jupyter)
        #  In this situation, we just use the default arguments
        parser = create_parser()
        args = parser.parse_args([])        
    print(args.exp_index)
    
    # Override arguments if we are using exp_index

    args_str = augment_args(args)
    
    # Set number of threads, if it is specified
    if args.cpus_per_task is not None:
        tf.config.threading.set_intra_op_parallelism_threads(args.cpus_per_task)
        tf.config.threading.set_inter_op_parallelism_threads(args.cpus_per_task)   
        
    #Importing Data, Many ways to do this.  Included is a data generator method 
    #using the function from suplement file (multiple methods available including
    # pickle files, folder structures, .mat files)     
    
    data,label = scipy.io.loadmat('clutter.mat')
    
    #Pull data given fold
    # dataset_train = create_dataset(base_dir=args.dataset,
    #                          partition='train', fold=args.rotation, filt=args.filts_train, 
    #                          batch_size=8, prefetch=2, num_parallel_calls=4)
    # dataset_valid = create_dataset(base_dir=args.dataset,
    #                          partition='train', fold=args.rotation, filt=args.filts_valid, 
    #                          batch_size=8, prefetch=2, num_parallel_calls=4)
    # dataset_test= create_dataset(base_dir=args.dataset,
    #                          partition='valid', fold=args.rotation, filt=args.filts_test, 
    #                          batch_size=8, prefetch=2, num_parallel_calls=4)

    # Parameter configuratuion
    if args.conv_size is not None:
        #dictionary of conv parameters
        conv_layers = [{'filters': f, 'kernel_size': s, 'stride': st}
                   for s, f, st, in zip(args.conv_size, args.conv_nfilters, args.conv_stride)]
    else:
        conv_layers = None  
    dense_layers = [{'units': i} for i in args.dense]
    
    #function to build model given layer parameters provided above
    model=create_cnn(args.image_size, args.nclasses, conv_layers=conv_layers,                             
                              maxpool=args.maxpool,
                              dense_layers=dense_layers,
                              activation='elu',
                              dropout=args.dropout_dense,
                              dropout_spatial=args.dropout_spatial,
                              lambda_l2=args.L2_dense,
                              lrate=args.lrate)

    
    # Report model structure if verbosity is turned on
    fbase = generate_fname(args, args_str)
    if args.verbose >= 1:
        print(model.summary())
        plot_model(model, to_file='%s_model_plot.png'%fbase, show_shapes=True, show_layer_names=True)
    print(args)

    # Output file base and pkl file

    fname_out = "%s_results.pkl"%fbase
    
    # Perform the experiment?
    if(args.nogo):
        # No!
        print("NO GO")
        print(fbase)
        return

    # Check if output file already exists
    if os.path.exists(fname_out):
            # Results file does exist: exit
            print("File %s already exists"%fname_out)
            return
            
    # Callbacks
    early_stopping_cb = keras.callbacks.EarlyStopping(patience=args.patience,
                                                      restore_best_weights=True,
                                                      min_delta=args.min_delta)

    # Learn
    #  steps_per_epoch: how many batches from the training set do we use for training in one epoch?
    history = model.fit(train_X, train_y,
                        epochs=args.epochs,
                        steps_per_epoch=args.steps_per_epoch,
                        use_multiprocessing=False, 
                        verbose=args.verbose>=2,
                        validation_data=(val_X,val_y),
                        validation_steps=args.steps_per_epoch, 
                        callbacks=[early_stopping_cb])

    # Calculate and store the important input/output and prediction information from the testing set
    results = {}
    results['args'] = args
    results['predict_validation'] = model.predict(val_X)
    results['predict_validation_eval'] = model.evaluate(val_X,val_y)
    
    if args.testing is not None:
        results['predict_testing'] = model.predict(test_X)
        results['predict_testing_eval'] = model.evaluate(test_X,test_y)
        
    results['predict_training'] = model.predict(train_X)
    results['predict_training_eval'] = model.evaluate(train_X,train_y)
    results['history'] = history.history

    
    # Save results
    fbase = generate_fname(args, args_str)
    results['fname_base'] = fbase
    with open("%s_results.pkl"%(fbase), "wb") as fp:
        pickle.dump(results, fp)
    
    # Save model
    print(fbase)
    model.save("%s_model"%(fbase))
   
    return model

if __name__ == "__main__":
    # Parse and check incoming arguments
    parser = create_parser()
    args = parser.parse_args()
    check_args(args)
    
    execute_exp(args)